1.spark优化?
答案：
架构参数优化：shuffle，内存管理，推测执行，数据本地化：HDFS的DataNode和Spark Worker共享一台机器
代码层面：并行度--调整finalRDD partition；缓存机制的选择--CPU使用和内存使用的权衡：	    checkpoint；算子的使用和选择-groupbykey，map vs mappartitions等，使用广播变量，累加器等； 序列化：压缩，存储格式的选择
数据倾斜：重写partition规则，抽样看数据的分布，结合具体的业务
架构的选择：统一使用yarn结合hadoop，还是使用自己的standalone计算框架

1.spark的工作流程？
答案：Spark的资源调度和任务调度+pipeline的计算模型

2.spark源码-DAG-Task--任务调度部分？
答案：
首先清楚spark是粗粒度的资源申请，任务调度：sparkContext-DAGSheduler切分stage，TaskSheduler发送任务到申请好的Executor中的线程池执行
3.submit相关配置？一般指定多大的资源？
答案：submit --master/yarn  --class  --deploy model clster/client
          --Executor cores	默认一个Executor 1 core，lg内存，1G，2--3个task
4.写完spark程序如何知道多少个task？	（即资源如何调配的）
答案：看你的并行度的设置，block的数量，web UI

5.spark和mr性能是不是差别很多？
答案：
一般来说Spark比Hadoop快：
原因：
（1）：MR有大量的磁盘io,溢写等，Spark则可以基于内存缓存机制计算
（2）：MR和Spark的资源申请的方式：粗粒度和细粒度的区别
（3）：DAG计算引擎中的pipeline计算模型，MR就是MapReduce模型
（4）：算子的丰富程度
使用场景：大于pb级别的数据量一般选择MR
生态的区别：Spark一站式的大数据处理平台，Hadoop还需要和其他的整合，升级，版本兼容等一堆问题，CDH版本如果需要更多的功能需要考虑成本的问题

6.spark任务yarn执行流程(client)?

7.spark运行在Yarn上流程（cluster）?
使用场景的区别：基于yarn的好处，兼容hadoop，一套计算框架，能好的维护

8.spark调优？

9.shuffle主要介绍下？
答案：shuffle发生？---shuffle的过程---shuffle实现的选择---shuffle的优化

10.宽窄依赖？
答案：看父RDD和子RR的关系，除了父RDD和子RDD一对多外，其他的都是窄依赖

11.shuffle怎么落地的？
答案：shuffle的实现类型：hash Shuffle还是sortShuffle？Shuffle数据落地？

12.Spark RDD 是什么?
答案：弹性分布式数据集---源码的五大特性-----RDD的计算模型：pipeline计算模型

13.Spark 算子?
答案：map，flatmap，filter，foreach，first，take(n),join,cogroup,reducebykey,sortBy,
distinct,mapPartition等等

14.spark 优势？
答案：
一栈式大数据处理平台。
灵活的编程模型，相比MR
速度快

15.spark on yarn  和mapreduce  中yarn有什么区别？
答案：没什么区别，yarn就是一个资源管理框架

16.spark 原理？
答案：pipeline计算模型+任务调度和资源调度

17.spark运行的job在哪里可以看到？
答案：Driver进程所在的节点；web UI

18.如何监测集群中cpu，内存的使用情况，比如说：有一个spark特别占资源，特别慢，怎么排查这种情况?
答案：Spark WEB UI；集群监控工具，找到taskid

19.spark为什么比hadoop快？同题5

20.rdd的处理过程是什么，不要说概念?
答案：画切分Stage，pipeline的计算模型的图

21.请说出你在spark中的优化方案？
答案：同1

22.SparkSQL和Spark架构，运行流程图，Spark运行的两种方式。常用的Spark函数有哪些？
答案：spark架构图+运行流程图(资源的调度+任务调度)+Spark client和SparkCluster+transformation算子+action算子+持久化操作算子

24：GroupByKey的作用？
答案：根据key分组

23.Spark了解多少？
答案：Spark生态-架构-运行模式+任务调度和资源调度



spark 遇到资源不足的时候怎么办？

spark代码提交
	脚本提交|平台提交

spark与MR的比较
  1、spark把运算的中间数据存放在内存，迭代计算效率更高；mapreduce的中间结果需要落地，需要保存到磁盘，
这样必然会有磁盘io操做，
     影响性能。


  2、spark容错性高，它通过弹性分布式数据集RDD来实现高效容错，RDD是一组分布式的存储在节点内存中的
只读性质的数据集，
     这些集合是弹性的，某一部分丢失或者出错，可以通过整个数据集的计算流程的血缘关系
来实现重建；mapreduce的话容错可能只能
     重新计算了，成本较高。


  3、spark更加通用，spark提供了transformation和action这两大类的多个功能api，另外还有流式处理
sparkstreaming模块、图计算GraphX
     等等；mapreduce只提供了map和reduce两种操作，流计算以及其他
模块的支持比较缺乏。
  

4、spark框架和生态更为复杂，有spark core,spark sql,sparkStreaming,sparkMLLib等（有RDD、血缘lineage、执行时的有向无环图DAG、
     stage划分等等），
很多时候spark作业都需要根据不同业务场景的需要进行调优已达到性能要求；mapreduce框架及其生态
相对较为简单，
     对性能的要求也相对较弱，但是运行较为稳定，适合长期后台运行。
  总结，spark生态更为丰富，功能更为强大、性能更佳，适用范围更广；mapreduce更简单、稳定性好、适合离线海量数据挖掘计算。

reduceByKey为什么比groupByKey执行快
  reduceByKey在map端会预聚合，groupByKey不会

Spark数据倾斜、GC调优？公司用那种GC策略
公司中Spark用的是哪种shuffle?

6.标签数据库?

8.数据倾斜在什么时候发生？
14.为什么会发生数据倾斜，你怎么知道发生数据倾斜的？

Spark中broadcast的传输过程

10.nginx了解吗？（没搞过，不负责）？

13.DSL和SQL用哪个比较多?

15.事务方面比较详细说下？

19.是否用过maven？
20.谈谈redis？
21.平实喜欢关注什么关键技术，论坛?

26.是否熟悉python，scala等等？

28.你的集群多大 每天流量多少？

----------------------------------------
Spark任务提交
29.你集群中定时任务是怎么做的？
你们公司中Spark任务在哪个资源框架上提交？说一下提交流程
----------------------------------------

36.什么不用hive呢？
37.ha原理？
38.远程通信技术？
39.对于宜信宜人贷平台的认识?
40.给与用户的每天通话数据，如何在五天内给出一个可行能够评判用户信任度的系统
主要是针对具体业务的技术解决方案，可靠性，备用方案等?
 我给了两种方案，人工标注部分数据做训练集，做分类算法，被否；然后给出聚类算法，从用户特征到聚类实现，可靠度评测等，面试官认为结果不准确，可以接入用户信息，并让我给出解决方案，这个讨论一半，面试官去开会，加了微信说是之后再谈，因为是猎头推荐的，后续猎头说继续跟进，暂时没有反馈
41.eclipse版本号?
42.大数据部门，说是让去独立负责这一块?
43.项目业务场景，问题，架构，人员分配，项目中遇到问题，规模等等----
再然后问了许多  为什么辞职，而且一直在追问具体情形（我回答是：希望换个环境，最后逼问没办法，只得回答说是，期望有一个更好的发展，然后说下自己的规划什么的）?
44.聊聊SOA，实现技术，最基本的特征啥的？

48.面试官问我以前的企业有多少人怎么说？问组内有多少人怎么说?问以前项目的数据量怎么说？问集群节点数量怎么说等等一系列比较菜鸟的问题，这些问题说重要倒不很重要，但是说不重要的话如果人家突然问到，你胡乱回答却容易闹笑话，让对方瞬间揭穿你毫无经验的事实。所以我的回答一般都是说用的测试数据，项目上线时候我本人没有参与。也许这么回答不是那么精妙，但是站我自己的角度感到最起码没有大的问题。（比如笔者第一家面试的时候，人家问我当初写的mapreduce程序运行一次多久，我说2小时。。。然后对方表示很震惊，问我多大数据量，我顿时碉堡了，感觉说多少都不合适，于是就找个机会岔开了一下话题，我说您是想问我mapreduce调优这一块吧？不等对方回答，我赶紧把背好mapreduce调优说了一下，来回胡扯了几句，结果竟然混过去了对方最后没再追问，还给了offer。。。第2家开始，所有面试问到运行了几分钟时候我一般会回答1到3分钟之间，项目用的测试数据，所以数据不大，节点的话就依照平时咱虚拟机装的节点规模稍微大一点
49.问了下openstack，我回答了下openstack的各个模块和作用
50.hbase+solr，让我说了一下，解释了一下，问solr你们怎么建索引的，建了多少索引，根据什么去建的这些索引？最后还问solr的索引表是单张表，还是多张表，这些表是存在hbase里面了还是分开的？为什么没有存hbase里面，搞得我无言以对。

52. hadoop与storm、spark的比较？?
53.事务都有哪些特点？
54.自我介绍？
55.问到了还使用过其他什么开源框架.？



60.平时遇到问题怎么解决?

63.Hadoop用的什么版本？他们公司用的是商业版?
64.PV UV IP解释?
65.数据倾斜问题怎么解决?
66.就问我会不会写PLSQL，好像只是在招写SQL的人?